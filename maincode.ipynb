{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deep569/bengali_question_answering_model-and-tokenizer_training/blob/main/maincode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "https://youtu.be/nsdCRVuprDY"
      ],
      "metadata": {
        "id": "JfmZ8TnZw3IT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "e1WesaQs3E_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "oJ30pbgHYe-p",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "N5kbpvaJYe0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"csebuetnlp/BanglaNMT\")"
      ],
      "metadata": {
        "id": "sf0m0jHUYeru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Replace 'dataset_name' with the name of your dataset, e.g., \"code_search_net\"\n",
        "dataset_name = \"csebuetnlp/BanglaNMT\"\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(dataset_name)\n",
        "\n",
        "# Get the keys of the first sample in the dataset\n",
        "keys = list(dataset[\"train\"][0].keys())\n",
        "\n",
        "# Print the keys\n",
        "print(keys)"
      ],
      "metadata": {
        "id": "Vu4P0nHjYeaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_corpus():\n",
        "    dataset = raw_datasets[\"train\"]\n",
        "    for start_idx in range(0, len(dataset), 1000):\n",
        "        samples = dataset[start_idx : start_idx + 1000]\n",
        "        yield samples[\"bn\"]"
      ],
      "metadata": {
        "id": "v2KAf3vxZrqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "training_corpus = get_training_corpus()\n",
        "new_tokenizer = old_tokenizer.train_new_from_iterator(training_corpus,52000)\n",
        "new_tokenizer.save_pretrained(\"Koustav_Tokenizer\")"
      ],
      "metadata": {
        "id": "eY3nWePxZwvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"গল্পটিতে\"\n",
        "\n",
        "print(old_tokenizer.tokenize(example))\n",
        "print(new_tokenizer.tokenize(example))"
      ],
      "metadata": {
        "id": "VGcwSqNrZxkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = [\n",
        "    1, 0.892857134, 0.952380955, 0.942857146, 0.833333313, 0.9375, 0.909090936,\n",
        "    0.962962985, 0.842105269, 0.869565189, 0.899999976, 0.692307711, 0.875,\n",
        "    0.863636374, 0.941176474, 0.862068951, 0.838709652, 0.896551728, 0.681818187,\n",
        "    0.918918908, 0.909090936, 0.800000012, 0.895833313, 0.761904776, 0.764705896,\n",
        "    0.961538434, 0.791666687, 0.949999988, 0.941176474, 0.714285731, 0.892857134,\n",
        "    0.714285731, 0.851851881, 0.777777791\n",
        "]\n",
        "\n",
        "# Sort the data in ascending order\n",
        "data.sort()\n",
        "\n",
        "# Calculate quartiles\n",
        "q1_index = int(len(data) * 0.25)\n",
        "q2_index = int(len(data) * 0.5)\n",
        "q3_index = int(len(data) * 0.75)\n",
        "\n",
        "q1 = data[q1_index]\n",
        "q2 = data[q2_index]\n",
        "q3 = data[q3_index]\n",
        "\n",
        "# Calculate interquartile range (IQR)\n",
        "iqr = q3 - q1\n",
        "\n",
        "# Calculate the lower and upper bounds for the interquartile range\n",
        "lower_bound = q1 - 1.5 * iqr\n",
        "upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "# Filter outliers\n",
        "filtered_data = [x for x in data if lower_bound <= x <= upper_bound]\n",
        "\n",
        "# Create the IQR graph\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.boxplot(filtered_data, vert=False)\n",
        "plt.xlabel('Accuracy')\n",
        "plt.title('Interquartile Range (IQR) Graph')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sjvVQhXxwqjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from statistics import mean, median, mode\n",
        "\n",
        "data = [\n",
        "    1, 0.892857134, 0.952380955, 0.942857146, 0.833333313, 0.9375, 0.909090936, 0.962962985,\n",
        "    0.842105269, 0.869565189, 0.899999976, 0.692307711, 0.875, 0.863636374, 0.941176474,\n",
        "    0.862068951, 0.838709652, 0.896551728, 0.681818187, 0.918918908, 0.909090936, 0.800000012,\n",
        "    0.895833313, 0.761904776, 0.764705896, 0.961538434, 0.791666687, 0.949999988, 0.941176474,\n",
        "    0.714285731, 0.892857134, 0.714285731, 0.851851881, 0.777777791\n",
        "]\n",
        "\n",
        "# Calculate mean, median, and mode\n",
        "data_mean = mean(data)\n",
        "data_median = median(data)\n",
        "data_mode = mode(data)\n",
        "\n",
        "# Create the histogram\n",
        "plt.hist(data, bins=10, edgecolor='black')\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('accuracy')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of the Data')\n",
        "\n",
        "# Add mean, median, and mode to the plot\n",
        "plt.axvline(x=data_mean, color='r', linestyle='dashed', linewidth=2, label='Mean')\n",
        "plt.axvline(x=data_median, color='g', linestyle='dashed', linewidth=2, label='Median')\n",
        "plt.axvline(x=data_mode, color='b', linestyle='dashed', linewidth=2, label='Mode')\n",
        "\n",
        "# Show legend\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "# Print the mean, median, and mode\n",
        "print(\"Mean:\", data_mean)\n",
        "print(\"Median:\", data_median)\n",
        "print(\"Mode:\", data_mode)\n",
        "\n"
      ],
      "metadata": {
        "id": "4HxXbk7sxQC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from statistics import mean, median, mode\n",
        "from scipy.stats import iqr\n",
        "\n",
        "data = [\n",
        "    1, 0.892857134, 0.952380955, 0.942857146, 0.833333313, 0.9375, 0.909090936, 0.962962985,\n",
        "    0.842105269, 0.869565189, 0.899999976, 0.692307711, 0.875, 0.863636374, 0.941176474,\n",
        "    0.862068951, 0.838709652, 0.896551728, 0.681818187, 0.918918908, 0.909090936, 0.800000012,\n",
        "    0.895833313, 0.761904776, 0.764705896, 0.961538434, 0.791666687, 0.949999988, 0.941176474,\n",
        "    0.714285731, 0.892857134, 0.714285731, 0.851851881, 0.777777791\n",
        "]\n",
        "\n",
        "# Calculate mean, median, and mode\n",
        "data_mean = mean(data)\n",
        "data_median = median(data)\n",
        "data_mode = mode(data)\n",
        "\n",
        "# Calculate interquartile range (IQR)\n",
        "data_iqr = iqr(data)\n",
        "\n",
        "# Create the interquartile plot\n",
        "plt.boxplot(data, vert=False)\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('Accuracy')\n",
        "plt.title('Interquartile Range (IQR) Plot')\n",
        "\n",
        "# Add mean, median, and mode to the plot\n",
        "plt.axvline(x=data_mean, color='r', linestyle='dashed', linewidth=2, label='Mean')\n",
        "plt.axvline(x=data_median, color='g', linestyle='dashed', linewidth=2, label='Median')\n",
        "plt.axvline(x=data_mode, color='b', linestyle='dashed', linewidth=2, label='Mode')\n",
        "\n",
        "# Add IQR box to the plot\n",
        "plt.axvline(x=data_median - (data_iqr / 2), color='k', linestyle='solid', linewidth=2, label='IQR Lower')\n",
        "plt.axvline(x=data_median + (data_iqr / 2), color='k', linestyle='solid', linewidth=2, label='IQR Upper')\n",
        "\n",
        "# Show legend\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "# Print the mean, median, mode, and IQR\n",
        "print(\"Mean:\", data_mean)\n",
        "print(\"Median:\", data_median)\n",
        "print(\"Mode:\", data_mode)\n",
        "print(\"IQR:\", data_iqr)\n"
      ],
      "metadata": {
        "id": "0qJp1Vwkyu3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir my"
      ],
      "metadata": {
        "id": "wUAS19ny89Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ok\n"
      ],
      "metadata": {
        "id": "D6uJknKR2Bvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "lXeZSEkmUvD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define your model (example)\n",
        "class YourModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(YourModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(100, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of your model\n",
        "model = YourModel()\n",
        "\n",
        "# Use AdamW optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the number of epochs\n",
        "num_epochs = 80\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Training steps\n",
        "    # Forward pass, backward pass, optimizer step, etc.\n",
        "    ...\n"
      ],
      "metadata": {
        "id": "xnQpM0Zimhzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U PyPDF2\n",
        "!pip install python-docx"
      ],
      "metadata": {
        "id": "l9PhAQicEyQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "PCs7rNylLjmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItYpaZD9EH7J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from PyPDF2 import PdfReader\n",
        "import os\n",
        "import docx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions to read different file types\n",
        "def read_pdf(file_path):\n",
        "    with open(file_path, \"rb\") as file:\n",
        "        pdf_reader = PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            text += pdf_reader.pages[page_num].extract_text()\n",
        "    return text\n",
        "\n",
        "def read_word(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    text = \"\"\n",
        "    for paragraph in doc.paragraphs:\n",
        "        text += paragraph.text + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def read_txt(file_path):\n",
        "    with open(file_path, \"r\") as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "def read_documents_from_directory(directory):\n",
        "    combined_text = \"\"\n",
        "    for filename in os.listdir(directory):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            combined_text += read_pdf(file_path)\n",
        "        elif filename.endswith(\".docx\"):\n",
        "            combined_text += read_word(file_path)\n",
        "        elif filename.endswith(\".txt\"):\n",
        "            combined_text += read_txt(file_path)\n",
        "    return combined_text\n"
      ],
      "metadata": {
        "id": "Ly_QfYPDHlie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_directory = '/content/my'\n",
        "text_data = read_documents_from_directory(train_directory)\n",
        "text_data = re.sub(r'\\n+', '\\n', text_data).strip()  # Remove excess newline characters\n"
      ],
      "metadata": {
        "id": "oobynnecHx87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/my/train.txt\", \"w\") as f:\n",
        "    f.write(text_data)"
      ],
      "metadata": {
        "id": "8RCPr_L0uJdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "TOJ6ApHY-keJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_samples = [\n",
        "    \"আমরা এ অঞ্চল এবং এর বাইরের অঞ্চলের মধ্যে সেতুবন্ধ প্রতিষ্ঠা করতে পারি।\"\n",
        "\"We can make bridge in this region and beyond the region.\"\n",
        "\"পররাষ্ট্র সচিব বলেন, দুই পক্ষের মধ্যে বিদ্যুৎ ব্যবসা সংক্রান্ত বিষয়েও আলোচনা হয়েছে যেহেতু ভুটানে জলবিদ্যুৎ উৎপাদন এবং এটি অঞ্চলে বাজারজাত করণের বিষয়ে আগে থেকেই আলোচনা চলছে।\"\n",
        "\"The foreign secretary said the two sides discussed the regional electricity trade as talks were already underway to produce hydropower in Bhutan and marketing that electricity in the region.\"\n",
        "\"বিষয়টি নিশ্চিত করে ঢাকা কেন্দ্রীয় কারাগারের জেলার মাহবুবুল ইসলাম বলেন, তিনি বিএসএমএমইউতে চিকিৎসা নিতে রাজি হয়েছেন।\"\n",
        "\"Confirming the matter, Mahbubul Islam, a jailer of Dhaka Central Jail said, She has agreed to take treatment at BSMMU.\"\n",
        "\"জাপান বাংলাদেশের জনগণের অন্তরে এক বিশেষ স্থান দখল করে আছে উল্লেখ করে তিনি বলেন, ১৯৭১ সালে আমাদের স্বাধীনতার পর থেকে জাপান যে পর্যায়ের প্রতিশ্রুতি প্রদর্শন করে আসছে তা সত্যিই অসাধারণ।\"\n",
        "\"Mentioning Japan occupies a very special place in the hearts of the people of Bangladesh she said, The level of commitment that Japan has been showing since our independence in 1971, is truly remarkable.\"\n",
        "\"উন্নয়নের এই ধারা অব্যাহত থাকলে এবং রাজনৈতিক স্থিতিশীলতা বজায় থাকলে প্রবৃদ্ধির এই গতি আরো ত্বরান্বিত হবে।\"\n",
        "\"This growth will accelerate if this trend of development and political stability continue.\"\n",
        "\"শেখ হাসিনা পুষ্পার্ঘ্য অর্পণের পর স্বাধীনতার স্থপতি বঙ্গবন্ধুর স্মৃতির প্রতি শ্রদ্ধা নিবেদনের অংশ হিসেবে সেখানে কিছুক্ষণ নীরবে দাঁড়িয়ে থাকেন।\"\n",
        "\"After laying the wreath, Sheikh Hasina stood in solemn silence for some time as a mark of profound respect to the memory of Bangabandhu, the architect of independence.\"\n",
        "\"গণভবনে ঈদের শুভেচ্ছা বিনিময় করবেন প্রধানমন্ত্রী\"\n",
        "\"Prime Minister to exchange Eid greetings at Ganabhaban\"\n",
        "\"নোটিশে সংশ্লিষ্ট কর্তৃপক্ষদেরকে ২৪ ঘণ্টার মধ্যে প্রয়োজনীয় পদক্ষেপ গ্রহণ করতে বলা হয়েছে।\"\n",
        "\"In the notice, the authorities concerned have been asked to take necessary steps within 24 hours.\"\n",
        "\"প্রধানমন্ত্রী ইতালিতে আন্তর্জাতিক কৃষি উন্নয়ন তহবিলের (ইফাদ) প্রেসিডেন্ট গিলবার্ট এফ হুয়াংবোর আমন্ত্রণে ইফাদের পরিচালনা পর্ষদের বার্ষিক অধিবেশনে যোগদান করেন।\"\n",
        "\"The premier attended the annual governing council meeting of the International Fund for Agricultural Development (IFAD) at the invitation of its President Gilbert F Houngbo in Italy.\"\n",
        "\"বাংলাদেশ একটি ধর্মনিরপেক্ষ রাষ্ট্র এবং সকল ধর্মের নাগরিক এদেশে সমান অধিকার নিয়ে বসবাস করবে।\"\n",
        "\"Bangladesh is a secular state and citizens of all religions will live in the country with similar rights.\"\n",
        "\"সোমবার একাদশ জাতীয় সংসদের পাঁচটি স্থায়ী কমিটি গঠন করা হয়েছে।\"\n",
        "\"The 11th Jatiya Sangsad on Monday formed five parliamentary standing committees.\"\n",
        "\"তিনি বলেন, তাদের জমি-সম্পত্তির ওপর অবশ্যই তাদের অধিকার থাকতে হবে।\"\n",
        "\"They must have an access to their land property, she said.\"\n",
        "\"লি কেকিয়াং এ সমস্যা দ্বিপক্ষীয় ভিত্তিতে সমাধানের ওপর গুরুত্ব আরোপ করেন এবং আশ্বস্থ করেন যে চীন এই সমস্যা সমাধানে সহায়তা করবে।\"\n",
        "\"Li Keqiang stressed the need for solving the crisis bilaterally and assured that China will help end the crisis.\"\n",
        "\"৪৯তম স্বাধীনতা দিবসকে 'বাংলাদেশ দিবস' ঘোষণা ওয়াশিংটন ডিসির\"\n",
        "\"Washington declares 49th Independence Day 'Bangladesh Day'\"\n",
        "\"আইনানুযায়ী জনবসতির কাছাকাছি ইটভাটা থাকার নিয়ম নেই।\"\n",
        "\"As per laws, there is no rule for setting up brick kilns adjacent to dwelling places.\"\n",
        "\"৭ জুন বিকেলে প্রধানমন্ত্রী ঢাকার উদ্দেশ্যে ফিনল্যান্ড ত্যাগ করবেন।\"\n",
        "\"The prime minister will leave Finland for home in the afternoon on June 7.\"\n",
        "\"সবার ঐক্যবদ্ধ প্রচেষ্টার মাধ্যমে একটি সুন্দর সোনার বাংলাদেশ গড়া তোলা সম্ভব।\"\n",
        "\"It is possible to build a beautiful golden Bangladesh through united efforts of all.\"\n",
        "\"সড়ক দুর্ঘটনা সাম্প্রতিক সময়ে উদ্বেগজনক পর্যায়ে পৌঁছেছে\"\n",
        "\"Road accidents have reached alarming level in recent times\"\n",
        "\"সংযুক্ত আরব আমিরাতে বাংলাদেশের রাষ্ট্রদূত মোহাম্মদ ইমরান এবং ঊর্ধ্বতন কর্মকর্তারা তাকে বিমানবন্দরে অভ্যর্থনা জানান।\"\n",
        "\"Bangladesh Ambassador to United Arab Emirates Mohammad Imran and senior officials received her at the airport.\"\n",
        "\"সৌদি আরব এবং বাংলাদেশের সশস্ত্র বাহিনীর মধ্যে চমৎকার সহযোগিতার কথা উল্লেখ করে জেনারেল ফায়াদ বলেন, দিন দিন এই সহযোগিতা বাড়ছে।\"\n",
        "\"Pointing out the excellent cooperation between Saudi Arab and Bangladesh Armed Forces, General Fayyadh said the cooperation is being enhanced day by day.\"\n",
        "\"আমরা মনে করি রপ্তানির এই প্রবৃদ্ধিতে ইতিবাচক প্রভাব পড়বে কর্মসংস্থান এবং অন্যান্য আর্থসামাজিক ক্ষেত্রে।\"\n",
        "\"We believe that this growth will have a positive impact on employment opportunity and other socio-economic fields.\"\n",
        "\"যুক্তরাজ্যে এই সফরে প্রধানমন্ত্রীর সফরসঙ্গী হিসেবে তার সঙ্গে অন্যান্যের মধ্যে রয়েছেন পররাষ্ট্রমন্ত্রী এ কে আব্দুল মোমেন, প্রধানমন্ত্রীর বেসরকারি শিল্প ও বিনিয়োগ উপদেষ্টা সালমান এফ রহমান ও পররাষ্ট্র প্রতিমন্ত্রী মো. শাহরিয়ার আলম।\"\n",
        "\"Foreign Minister AK Abdul Momen, Prime Minister's Private Industry and Investment Advisor Salman F Rahman and State Minister for Foreign Affairs Md. Shahriar Alam, among others, are accompanying the prime minister during her visit to the UK.\"\n",
        "\"এর আগে অফিস সময়ে সরকারি চিকিৎসকদের প্রাইভেট প্র্যাকটিস বন্ধে গত ২৯ জানুয়ারি সংশ্লিষ্টদের আইনি নোটিশ দেন সুপ্রিম কোর্টের আইনজীবী আব্দুস সাত্তার পালোয়ান।\"\n",
        "\"Earlier on January 29, Supreme Court lawyer Abdus Sattar Palwan served a legal notice to the authorities concerned asking to stop private practice of the government doctors during their office time.\"\n",
        "\"আবদুল হামিদ দীর্ঘদিন ধরেই গ্লুকোমার সমস্যায় ভুগছেন।\"\n",
        "\"Abdul Hamid has been suffering from Glaucoma since long.\"\n",
        "\"বুধবার বিকেলে স্বরাষ্ট্র মন্ত্রনালয়েরর জননিরাপত্তা বিভাগ ডিএমপির নতুন কমিশনার হিসেবে শফিকুল ইসলামের নাম ঘোষণা করে এক প্রজ্ঞাপন জারি করে।\"\n",
        "\"Public Security Division of Home ministry issued a gazette notification announcing Shafiqul Islam as the new DMP commissioner on Wednesday afternoon.\"\n",
        "\"এ প্রকল্প আমাদেরই এবং এটি আমাদেরই বাস্তবায়ন করতে হবে।\"\n",
        "\"This is our project and we will have to implement it.\"\n",
        "\"বহুল প্রতীক্ষিত পদ্মা সেতু রেল সংযোগ প্রকল্পের ঋণ চুক্তি স্বাক্ষরিত হয়েছে।\"\n",
        "\"The loan agreement of the much-expected Padma Bridge rail link project has been signed.\"\n",
        "\"তাই এর কর্মকর্তা ও কর্মচারীদের জনগণকে সেবার মানসিকতা নিয়ে কাজ করতে হবে।\"\n",
        "\"Therefore, its officers and employees have to work with the mentality of serving the people.\"\n",
        "\"বাংলাদেশ ও ভুটানের সম্পর্ককে তিনি 'গভীর এবং ঐতিহাসিক' বলে অভিহিত করেন কেননা ১৯৭১ সালের ৬ ডিসেম্বর ভুটানই সর্বপ্রথম বাংলাদেশকে আনুষ্ঠানিক স্বীকৃতি প্রদান করে।\"\n",
        "\"He described Bangladesh ties with Bhutan to be deep and historic as Bhutan was first to recognize Bangladesh on December 6 in 1971.\"\n",
        "\"১৮ মার্চ শুরু হওয়া এই মহড়ায় বাংলাদেশ সশস্ত্রবাহিনীর ১৮ সদস্যের একটি প্রতিনিধিদল অংশ নিয়েছে।\"\n",
        "\"An 18-member delegation of Bangladesh Armed Forces was participating in the exercise that began on March 18.\"\n",
        "\"দেশে ফেরার সময় রাষ্ট্রপতি দুই দিনের সফরে সিঙ্গাপুর যাবেন।\"\n",
        "\"On his way back home, the President will make a two-day stopover in Singapore.\"\n",
        "\"আওয়ামী লীগের সাংগঠনিক সম্পাদক ও সংসদ সদস্য আব্দুল মান্নানের মৃত্যুতে গভীর শোক ও দুঃখ প্রকাশ করেছেন প্রধানমন্ত্রী শেখ হাসিনা।\"\n",
        "\"Prime Minister Sheikh Hasina expressed deep shock and sorrow at the death of organising secretary to Awami League and Member of the Parliament Abdul Mannan.\"\n",
        "\"বিশ্ববাসীকে এই সংকট মোকাবেলায় বাংলাদেশের পাশে দাঁড়ানোর আহ্বান জানান তিনি।\"\n",
        "\"He urged the world people to stand beside Bangladesh to face this crisis.\"\n",
        "\"কৃষকদের সুযোগ-সুবিধা বাড়ানোর চেষ্টা চলছে, যাতে পেঁয়াজের আবাদ বাড়ে।\"\n",
        "\"Initiatives have been taken to increase facilities of the farmers, so that onion cultivation is increased.\"\n",
        "\"প্রধানমন্ত্রী শেখ হাসিনার সঙ্গে বৈঠক করেছেন সফররত দক্ষিণ কোরিয়ার প্রধানমন্ত্রী লি নাক-ইয়োন।\"\n",
        "\"Visiting South Korean Prime Minister Lee Nak-yon has held a meeting with Prime Minister Sheikh Hasina.\"\n",
        "\"এ ছাড়া প্রধানমন্ত্রী অছি পরিষদে ইউনিভার্সাল হেলথ কভারেজ বিষয়ে উচ্চ পর্যয়ের বৈঠকের পূর্ণাঙ্গ অধিবেশনে দেশের অবস্থান তুলে ধরবেন বলে আশা করা হচ্ছে।\"\n",
        "\"Besides, the prime minister is expected to present the national statement at the plenary session of the high-level meeting on universal health coverage at Trusteeship Council.\"\n",
        "\"শ্রেণিবিন্যাসের প্রক্রিয়াটি অবশ্যই স্বচ্ছ হতে হবে এবং প্রকৃতপক্ষে সারাক্ষণই যৌক্তিক প্রতিপন্নতা এবং সামাজিক তদন্তের জন্যে উন্মুক্ত রাখতে হবে।\"\n",
        "\"The process of categorization must be transparent, and indeed, subject to constant rational and open scrutiny by society.\"\n",
        "\"আন্তঃবাহিনী জনসংযোগ পরিদপ্তরের (আইএসপিআর) সহকারী পরিচালক রাশেদুল আলম খান বিষয়টি নিশ্চিত করেন।\"\n",
        "\"Rashedul Alam Khan, Assistant Director of the Inter Services Public Relations (ISPR), confirmed the matter.\"\n",
        "\"জাপানের সহায়তায় বাংলাদেশে বাস্তবায়নাধীন বিভিন্ন মেগা প্রকল্পের উল্লেখ করে জাপানের রাষ্ট্রদূত বলেন, বাংলাদেশ জাপানের সব থেকে দীর্ঘ এবং বৃহৎ উন্নয়ন সহযোগী।\"\n",
        "\"Pointing out various mega projects being executed in Bangladesh by the Japanese assistance, the ambassador said Bangladesh is the longest and largest development partner of Japan.\"\n",
        "\"পরে পুলিশ ওই বাক্স থেকে সালামা আক্তারের লাশ উদ্ধার করে।\"\n",
        "\"Later, police recovered Salma Akhter's body from the box.\"\n",
        "\"বাংলাদেশে তৈরি পণ্যের বৈশ্বিক চাহিদা রয়েছে।\"\n",
        "\"There is a global demand for products made in Bangladesh.\"\n",
        "\"অ্যাটর্নি জেনারেল তাদেরকে বৃহস্পতিবার দুপুরে তার কার্যালয়ে ডেকে পদত্যাগ করতে নির্দেশ দেন।\"\n",
        "\"The Attorney General called them to his office on Thursday noon and instructed them to resign.\"\n",
        "\"অনুষ্ঠানে জুনাইদ আহমেদ পলক বলেন, বাংলাদেশে তৈরি পণ্য আমেরিকার বাজারে যাচ্ছে, এটা শুধু ওয়ালটনের জন্য ঐতিহাসিক দিন নয়, বরং বাংলাদেশের জন্যও ঐতিহাসিক দিন।\"\n",
        "\"Speaking on the occasion, Zunaid Ahmed Palak said, products made in Bangladesh are going to the American market, this is not only a historic day for Walton, but also a historic day for Bangladesh as well.\"\n",
        "\"সমস্যা সমাধানে পরিবেশ সংরক্ষণ আইনের যথাযথ প্রয়োগ ও বাস্তবায়ন নিশ্চিত করতে পরিবেশ অধিদপ্তরের কার্যকর ব্যবস্থা গ্রহণ করা প্রয়োজন।\"\n",
        "\"Department of Envireonment needs to take effective steps to ensure proper application and implementation of Environment Protection Act to address the problems.\"\n",
        "\"দক্ষিণ এশিয়ায় আমাদের জ্ঞান, অভিজ্ঞতা, দক্ষতা ও বিনিয়োগের মাধ্যমে অন্যদের হাত ধরতে হবে।\"\n",
        "\"We need to hold hands of others across South Asia through our knowledge, experience, expertise and investments.\"\n",
        "\"প্রধানমন্ত্রী শেখ হাসিনা সংযুক্ত আরব আমিরাতে (ইউএই) তিন দিনের সরকারি সফর শেষে আজ রাতে দেশে ফিরেছেন।\"\n",
        "\"Prime Minister Sheikh Hasina returned home tonight, concluding her three-day official visit to the United Arab Emirates (UAE).\"\n",
        "\"ফ্লাইটটি স্থানীয় সময় রাত ৮টা ৫৫ মিনিটে আবুধাবি আন্তর্জাতিক বিমানবন্দরে অবতরণ করার কথা রয়েছে যেখানে সংযুক্ত আরব আমিরাতে নিযুক্ত বাংলাদেশের রাষ্ট্রদূত মোহম্মদ ইমরান প্রধানমন্ত্রীকে স্বাগত জানাবেন।\"\n",
        "\"The flight is scheduled to reach Abu Dhabi International Airport at 8:55pm local time where Bangladesh Ambassador to the United Arab Emirates Muhammad Imran will receive the premier at the airport.\"\n",
        "\"তিনি বলেন, এতে কোনো সন্দেহ নেই যে, এটা বাংলাদেশের জন্য একটি বড় সমস্যা।\"\n",
        "\"There is no doubt that it's a big problem for Bangladesh, he said.\"\n",
        "\n",
        "]\n",
        "\n",
        "val_file_path = \"/content/my/val.txt\"\n",
        "\n",
        "with open(val_file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(\"\\n\".join(val_samples))"
      ],
      "metadata": {
        "id": "iEXNKHAGlztw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BengaliTokenizer\n",
        "# AutoTokenizer\n",
        "\n",
        "from transformers import AutoTokenizer, GPT2DoubleHeadsModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from transformers import EvalPrediction\n",
        "import torch\n",
        "import csv\n",
        "\n",
        "\n",
        "\n",
        "def load_dataset(train_file_path, val_file_path, tokenizer, block_size=128):\n",
        "    train_dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=train_file_path,\n",
        "        block_size=block_size,\n",
        "    )\n",
        "    val_dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=val_file_path,\n",
        "        block_size=block_size,\n",
        "    )\n",
        "    return train_dataset, val_dataset\n",
        "def load_data_collator(tokenizer, mlm=False):\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=mlm,\n",
        "    )\n",
        "    return data_collator  # Return the same data collator for both train and validation\n",
        "\n",
        "import csv\n",
        "import torch\n",
        "\n",
        "def compute_accuracy(labels_ids, preds_ids):\n",
        "    accuracy = (preds_ids == labels_ids).mean()\n",
        "    return accuracy.item()\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    labels_ids = p.label_ids\n",
        "    preds_ids = p.predictions.argmax(-1)\n",
        "    accuracy = compute_accuracy(labels_ids, preds_ids)\n",
        "\n",
        "    # Save the labels, predictions, and accuracy to a CSV file\n",
        "    with open(\"accuracy.csv\", \"w\", newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile, delimiter=\",\")\n",
        "        writer.writerow([\"Labels\", \"Predictions\", \"Accuracy\"])\n",
        "        for label, prediction in zip(labels_ids, preds_ids):\n",
        "            writer.writerow([label.tolist(), prediction.tolist(), accuracy])\n",
        "\n",
        "    return {\"accuracy\": accuracy}\n",
        "\n",
        "\n",
        "\n",
        "def train(train_file_path, val_file_path, model_name,\n",
        "          output_dir,\n",
        "          overwrite_output_dir,\n",
        "          per_device_train_batch_size,\n",
        "          num_train_epochs,\n",
        "          save_steps, learning_rate):\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"Koustav_Tokenizer\")\n",
        "    train_dataset, val_dataset = load_dataset(train_file_path, val_file_path, tokenizer)\n",
        "    data_collator = load_data_collator(tokenizer)  # Use the same data collator for both training and validation\n",
        "\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    model = GPT2DoubleHeadsModel.from_pretrained(model_name)\n",
        "\n",
        "    model.save_pretrained(output_dir)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        overwrite_output_dir=overwrite_output_dir,\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        learning_rate=learning_rate,\n",
        "        evaluation_strategy=\"steps\",  # Evaluate at specified steps\n",
        "        eval_steps=save_steps,\n",
        "        logging_steps=20, # Evaluate every `save_steps` during training\n",
        "    )\n",
        "\n",
        "    import csv\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,  # Set the validation dataset\n",
        "        compute_metrics=compute_metrics,\n",
        "       # Set the custom compute_metrics function\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model()\n",
        "\n",
        "\n",
        "# ... (rest of the code)\n",
        "\n",
        "\n",
        "\n",
        "train_file_path = \"/content/my/train.txt\"\n",
        "val_file_path = \"/content/my/val.txt\"\n",
        "model_name = 'gpt2-medium'\n",
        "output_dir = '/content/my'\n",
        "overwrite_output_dir = False\n",
        "per_device_train_batch_size = 16\n",
        "num_train_epochs = 100\n",
        "save_steps = 20\n",
        "\n",
        "# Add a parameter to adjust the learning rate\n",
        "learning_rate = float(input(\"Enter the learning rate: \"))\n",
        "\n",
        "train(\n",
        "    train_file_path=train_file_path,\n",
        "    val_file_path=val_file_path,\n",
        "    model_name=model_name,\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=overwrite_output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    save_steps=save_steps,\n",
        "    learning_rate=learning_rate,\n",
        ")\n"
      ],
      "metadata": {
        "id": "rkStEuxUidyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "xwGS1IMlGBMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "dWyzGAHTw2y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from transformers import EvalPrediction\n",
        "import csv\n",
        "\n"
      ],
      "metadata": {
        "id": "lelq_sN4Gy5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_path):\n",
        "    model = GPT2DoubleHeadsModel.from_pretrained(model_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_tokenizer(tokenizer_path):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"Koustav_Tokenizer\")\n",
        "    return tokenizer\n",
        "\n",
        "def generate_text(model_path, prompt, max_new_tokens=50):\n",
        "    model = load_model(model_path)\n",
        "    tokenizer = load_tokenizer(model_path)\n",
        "    ids = tokenizer.encode(f'{prompt}', return_tensors='pt')\n",
        "    # Set max_length to a very large value to allow unrestricted text generation\n",
        "    final_outputs = model.generate(\n",
        "        ids,\n",
        "        do_sample=True,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        pad_token_id=model.config.eos_token_id,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        temperature=0.5,\n",
        "        early_stopping=True,  # Prevent the model from generating more text after the first end-of-sequence token\n",
        "         num_return_sequences=1,\n",
        "    )\n",
        "    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))\n",
        "\n"
      ],
      "metadata": {
        "id": "wOvrNQRAG2IP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model got trained on the entire text and took much longer to train, and yet it fails to give meaningful results."
      ],
      "metadata": {
        "id": "BvNx7gjeRieD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1_path = \"/content/my\"\n",
        "\n",
        "prompt= \"হ্যালো, আপনি কেমন আছেন:\"\n",
        "max_new_tokens=100\n",
        "\n",
        "generate_text(model1_path,prompt, max_new_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mTDTrpnG5Ut",
        "outputId": "a12c5f68-a752-47a7-8d68-a09a046faee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "হ্যালো, আপনি কেমন আছেন: ফোনিয়ার প্রাধ্যক্ষ ড. জিয়া রহমান বিষয়টি নিশ্চিত করেন।\"\t\n",
            "\"Shafiqul Islam, Chief of Criminal Investigation Departmen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZHIECRog5kBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2DoubleHeadsModel, AutoTokenizer\n",
        "import csv\n",
        "\n",
        "def calculate_accuracy(logits, targets):\n",
        "    predicted_tokens = logits.argmax(dim=-1)\n",
        "    accuracy = (predicted_tokens == targets).float().mean()\n",
        "    return accuracy\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your trained GPT-2 model and tokenizer (change the paths accordingly)\n",
        "    model_path = \"/content/my\"\n",
        "    tokenizer_path = \"/content/Koustav_Tokenizer\"\n",
        "    model = GPT2DoubleHeadsModel.from_pretrained(model_path)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "\n",
        "    # Initialize a list to store user inputs and their respective accuracies\n",
        "    user_inputs = []\n",
        "    accuracies = []\n",
        "\n",
        "    # Take at least 50 user inputs\n",
        "    while len(user_inputs) < 2:\n",
        "        user_input = input(\"Enter your text: \")\n",
        "        if user_input.strip():\n",
        "            user_inputs.append(user_input)\n",
        "\n",
        "            # Tokenize the user input and return PyTorch tensors\n",
        "            test_inputs = tokenizer([user_input], return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "            # Evaluate the model and calculate accuracy\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                input_ids = test_inputs[\"input_ids\"].unsqueeze(0)\n",
        "                outputs = model(input_ids)[0]\n",
        "\n",
        "                # Get the predicted token probabilities\n",
        "                predicted_probs = F.softmax(outputs, dim=-1)\n",
        "\n",
        "                # Get the predicted tokens\n",
        "                predicted_tokens = predicted_probs.argmax(dim=-1).squeeze()\n",
        "\n",
        "                # Get the target tokens\n",
        "                target_tokens = input_ids.squeeze()\n",
        "\n",
        "                # Calculate the accuracy for this example\n",
        "                accuracy = calculate_accuracy(predicted_tokens, target_tokens)\n",
        "\n",
        "                # Append the accuracy to the list\n",
        "                accuracies.append(accuracy.item())\n",
        "\n",
        "    # Save the user inputs and accuracies to a CSV file\n",
        "    with open(\"user_accuracies.csv\", \"w\", newline=\"\") as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([\"User Input\", \"Accuracy\"])\n",
        "        for user_input, accuracy in zip(user_inputs, accuracies):\n",
        "            writer.writerow([user_input, accuracy])\n",
        "\n",
        "    print(\"User inputs and accuracies have been saved to 'user_accuracies.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHWNgdocHFbZ",
        "outputId": "da52a14d-3e96-41d9-942b-4b0db34460f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your text: পোরকুপাইনরা কী খায়\t\n",
            "Enter your text: exit\n",
            "User inputs and accuracies have been saved to 'user_accuracies.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def calculate_accuracy(logits, targets):\n",
        "  \"\"\"Calculates the accuracy of a sequence of logits.\n",
        "\n",
        "  Args:\n",
        "    logits: A tensor of logits.\n",
        "    targets: A tensor of targets.\n",
        "\n",
        "  Returns:\n",
        "    The accuracy of the sequence.\n",
        "  \"\"\"\n",
        "\n",
        "  # Get the predicted token probabilities (ignore the [CLS] token at the beginning)\n",
        "  predicted_probs = F.softmax(logits[:, :-1], dim=-1)\n",
        "\n",
        "  # Get the predicted tokens (ignore the [CLS] token at the beginning)\n",
        "  predicted_tokens = predicted_probs.argmax(dim=-1)\n",
        "\n",
        "  # Get the target tokens (ignore the [CLS] token at the beginning)\n",
        "  target_tokens = targets[:, 1:]\n",
        "\n",
        "  # Calculate the accuracy\n",
        "  accuracy = (predicted_tokens == target_tokens).float().mean()\n",
        "  return accuracy\n",
        "\n",
        "  import transformers\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # Load your GPT-2 model (change the model_name_or_path as per your model)\n",
        "  model =  GPT2LMHeadModel.from_pretrained(model_path)\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"Koustav_Tokenizer\")\n",
        "\n",
        "  # Set a padding token for the tokenizer\n",
        "  tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "  # Prepare your test data and labels\n",
        "  test_texts = [\n",
        "   \"হিমবাহ গুহা কিভাবে গঠিত হয়?\"\n",
        "  ]\n",
        "\n",
        "  # Tokenize the test_texts and return PyTorch tensors\n",
        "  test_inputs = tokenizer(test_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "  # Evaluate the model and calculate accuracy\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    accuracy = 0\n",
        "    for i in range(len(test_texts)):\n",
        "      input_ids = test_inputs[\"input_ids\"][i].unsqueeze(0)\n",
        "      outputs = model(input_ids)[0]\n",
        "\n",
        "      # Get the predicted token probabilities (ignore the [CLS] token at the beginning)\n",
        "      predicted_probs = F.softmax(outputs[:, :-1], dim=-1)\n",
        "\n",
        "      # Get the predicted tokens (ignore the [CLS] token at the beginning)\n",
        "      predicted_tokens = predicted_probs.argmax(dim=-1)\n",
        "\n",
        "      # Get the target tokens (ignore the [CLS] token at the beginning)\n",
        "      target_tokens = input_ids[0, 1:]\n",
        "\n",
        "      # Calculate the accuracy\n",
        "      accuracy += (predicted_tokens == target_tokens).float().mean()\n",
        "\n",
        "  # Print the accuracy\n",
        "  print(\"Accuracy:\", accuracy / len(test_texts))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYxaQfeSItjx",
        "outputId": "b56b092d-4870-4cff-f7e5-5532d4b4d616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: tensor(1.)\n"
          ]
        }
      ]
    }
  ]
}